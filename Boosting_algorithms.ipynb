{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting\n",
        "\n",
        "After we use our regression model to estimate the dependent variable with input features, we will have residuals.\n",
        "\n",
        "$$residuals_i=\\beta_ix_i - y_i$$\n",
        "\n",
        "To improve the accuracy of predictions by the initial model (we considered as a weak learner), scientists introduced a method to train another model on the error terms with respect to x. This is gradient boosting. The most common approach is to train a decision tree as the boosting algorithm, but there are many choices of boosters. The final prediction after gradient booster will be the sum of predictions made by the weak learner and prediction made by boosting algorithm.\n"
      ],
      "metadata": {
        "id": "KoUAWenxuTkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost\n",
        "\n",
        "XGBoost, or Extreme gradient boosting, uses regularization parameters to avoid overfitting. Same as gradient boosting discussed above, XGBoost can use deicision tree based boosting method.\n",
        "\n",
        "After we fit the original model, we calculate the residuals for each data point. We then implement decision tree to the residuals. The methods for splits is maximizing __Gain__, which is the improvement in accuracy. There are two important hyperparameter in the Gain function to control how we split the tree: $\\lambda$, and $\\gamma$. $\\lambda$ is used to avoids overly sensitivity to individual data points, and $\\gamma$ is the threshold to stop further splitting the tree.\n",
        "\n",
        "The function for Gain is applied with each split:\n",
        "\n",
        "$$\\text{Gain} = \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} - \\gamma$$\n",
        "\n",
        "\n",
        "$G_L$ is the sum of residuals in the left node, $G_R$ is the sum of residuals in the right node, $H_R$ is the number of resuiduals in the left node and $H_L$ is the number of residuals in the right node.\n",
        "\n",
        "For each node, we use the Gain function to decide where is the best place to split the tree. The node wil be splitted at where the Gain function achieve the highest value. The algorithm will examine each possible way of splitting the tree. The tree will stop further developing when the Gain function outputs negative values. Each node will be examined and splitted until all ways of splitting will yield negative values for splitting.\n",
        "\n",
        "After the first decision tree is constructed based on the residuals, we have a new model. The prediction now equals to the initial prediction plus the prediction made by the decision tree learner multiplies a learning rate. The new model will have a new residuals when it is compared to the real values of y. The XGBoost algorithm will build another decision tree based on the new residuals. This process is repeated n times(a designated values). The final prediction will be as follows\n",
        "\n",
        "\\text{Prediction} = \\text{Initial Prediction} + \\text{Learning Rate} \\times \\text{Prediction}_1 + \\text{Learning Rate} \\times \\text{Prediction}_2 + \\ldots + \\text{Learning Rate} \\times \\text{Prediction}_n\n"
      ],
      "metadata": {
        "id": "lp0bz1U1Gc4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$Prediction = Initial Prediction + Learning Rate \\times Prediction_1 + Learning Rate \\times Prediction_2 + ....Learning Rate \\times Prediction_n$$"
      ],
      "metadata": {
        "id": "H0M8EsvrS7uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I will show the steps in writing a gradient boosting algorithm with a random forest regressor as booster. Then I will applied the gradient boosted regression algorithm to the analysis of a real dataset and compare it with prediction made by the original regression algorithm and xgboost. The performance of these models will be evaluated by a cross-validated mean square error.\n",
        "\n",
        "I will the Boston housing dataset. Features will be used to predict cmedv, the mean price of the house. I will first use a locally weighted regression model to fit the data. The locally weighted regression model was introduced in last project. Then, I will use a random forest model to boost the locally weighted regression model.\n",
        "\n"
      ],
      "metadata": {
        "id": "M7QRZfRjKEyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.linalg import lstsq\n",
        "from scipy.sparse.linalg import lsmr\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold, train_test_split as tts\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "ciL1G_oTV7Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define locally weighted regression\n",
        "def lwr_reg(X, y, xnew, kern, tau, intercept):\n",
        "\n",
        "    n = len(X)\n",
        "    yest = np.zeros(n)\n",
        "\n",
        "    if len(y.shape)==1:\n",
        "      y = y.reshape(-1,1)\n",
        "\n",
        "    if len(X.shape)==1:\n",
        "      X = X.reshape(-1,1)\n",
        "\n",
        "    if intercept:\n",
        "      X1 = np.column_stack([np.ones((len(X),1)),X])\n",
        "    else:\n",
        "      X1 = X\n",
        "\n",
        "    w = np.array([kern((X - X[i])/(2*tau)) for i in range(n)])\n",
        "\n",
        "    for i in range(n):\n",
        "        W = np.diag(w[:,i])\n",
        "        b = np.transpose(X1).dot(W).dot(y)\n",
        "        A = np.transpose(X1).dot(W).dot(X1)\n",
        "        theta, res, rnk, s = lstsq(A, b)\n",
        "        yest[i] = np.dot(X1[i],theta)\n",
        "    if X.shape[1]==1:\n",
        "      f = interp1d(X.flatten(),yest,fill_value='extrapolate')\n",
        "    else:\n",
        "      f = LinearNDInterpolator(X, yest)\n",
        "    output = f(xnew)\n",
        "    if sum(np.isnan(output))>0:\n",
        "      g = NearestNDInterpolator(X,y.ravel())\n",
        "      output[np.isnan(output)] = g(xnew[np.isnan(output)])\n",
        "    return output"
      ],
      "metadata": {
        "id": "6YurGeXcVQqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the kernel we use for the locally weighted regression\n",
        "# Tricubic Kernel\n",
        "def Tricubic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,70/81*(1-d**3)**3)\n",
        "\n",
        "# Quartic Kernel\n",
        "def Quartic(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,15/16*(1-d**2)**2)\n",
        "\n",
        "# Epanechnikov Kernel\n",
        "def Epanechnikov(x):\n",
        "  if len(x.shape) == 1:\n",
        "    x = x.reshape(-1,1)\n",
        "  d = np.sqrt(np.sum(x**2,axis=1))\n",
        "  return np.where(d>1,0,3/4*(1-d**2))"
      ],
      "metadata": {
        "id": "sLUdPk_8VQtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define evaluation methods\n",
        "def CrossMSE(X,y,kernel,tau,intercept):\n",
        "  mse_i = []\n",
        "  kf = KFold(n_splits=10,shuffle=True,random_state=123)\n",
        "  for idxtrain, idxtest in kf.split(X):\n",
        "      xtrain = X[idxtrain]\n",
        "      ytrain = y[idxtrain]\n",
        "      ytest = y[idxtest]\n",
        "      xtest = X[idxtest]\n",
        "      xtrain = scale.fit_transform(xtrain)\n",
        "      xtest = scale.transform(xtest)\n",
        "      yhat = lwr_reg(xtrain,ytrain,xtest,kernel,tau,intercept)\n",
        "      mse_i.append(mse(ytest,yhat))\n",
        "  return np.mean(mse_i)"
      ],
      "metadata": {
        "id": "tjyuLwHiXTso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data410/Non-linearReg/BostonHousingPrices.csv')\n",
        "#define dependent and independent variables\n",
        "x = data[['crime', 'ptratio', 'rooms', 'older','lstat']].values\n",
        "y = data['cmedv'].values"
      ],
      "metadata": {
        "id": "AydeePJFVQvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scale = StandardScaler()\n",
        "x_scaled = scale.fit_transform(x)"
      ],
      "metadata": {
        "id": "VHEzKzSHVQ0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_lwr = CrossMSE(x_scaled,y,Tricubic,0.5,True)\n",
        "print('The cross-validated mean square error of locally weighted regression is '+str(mse_lwr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ms_3AaU7Ycrm",
        "outputId": "8a6baf1c-417b-46b6-b397-6d66146faff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cross-validated mean square error of locally weighted regression is 17.817376346186034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here is the function of gradient boosted locally weighted regression model\n",
        "def boosted_lwr(X, y, xnew, kern, tau, intercept):\n",
        "  y_initial= lwr_reg(X,y,X,kern,tau,intercept)\n",
        "  new_y = y - y_initial\n",
        "  model = RandomForestRegressor(n_estimators=100,max_depth=2)\n",
        "  model.fit(X,new_y)\n",
        "  output = model.predict(xnew) + lwr_reg(X,y,xnew,kern,tau,intercept)\n",
        "  return output"
      ],
      "metadata": {
        "id": "LbUrTN_2ZKtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossMSE_Boosted(X,y,kernel,tau,intercept):\n",
        "  mse_i = []\n",
        "  kf = KFold(n_splits=10,shuffle=True,random_state=123)\n",
        "  for idxtrain, idxtest in kf.split(X):\n",
        "      xtrain = X[idxtrain]\n",
        "      ytrain = y[idxtrain]\n",
        "      ytest = y[idxtest]\n",
        "      xtest = X[idxtest]\n",
        "      xtrain = scale.fit_transform(xtrain)\n",
        "      xtest = scale.transform(xtest)\n",
        "      yhat = boosted_lwr(xtrain,ytrain,xtest,kernel,tau,intercept)\n",
        "      mse_i.append(mse(ytest,yhat))\n",
        "  return np.mean(mse_i)"
      ],
      "metadata": {
        "id": "qh2N8uYlaR_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_gboost = CrossMSE_Boosted(x,y,Tricubic,0.5,True)"
      ],
      "metadata": {
        "id": "NZ73l1BBaCaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_lwr = CrossMSE(x_scaled,y,Tricubic,0.5,True)\n",
        "print('The cross-validated mean square error of locally weighted regression with gradient boosting is '+str(mse_gboost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW5M6I4dUjgG",
        "outputId": "1eee994c-3e93-4558-da01-9bfc287dfb0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cross-validated mean square error of locally weighted regression with gradient boosting is 17.728742021954826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a slight improvement of accuracies after the locally weighted regression model is boosted with random forest."
      ],
      "metadata": {
        "id": "v_kxJHUnK9vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "1ZMdKxcdXNDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_xgb = xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=100,reg_lambda=20,alpha=1,gamma=10,max_depth=3)"
      ],
      "metadata": {
        "id": "q6gvmXLnXT2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CrossMSE_xgb(X,y):\n",
        "  mse_i = []\n",
        "  kf = KFold(n_splits=10,shuffle=True,random_state=123)\n",
        "  for idxtrain, idxtest in kf.split(X):\n",
        "      xtrain = X[idxtrain]\n",
        "      ytrain = y[idxtrain]\n",
        "      ytest = y[idxtest]\n",
        "      xtest = X[idxtest]\n",
        "      xtrain = scale.fit_transform(xtrain)\n",
        "      xtest = scale.transform(xtest)\n",
        "      model_xgb.fit(xtrain,ytrain)\n",
        "      yhat = model_xgb.predict(xtest)\n",
        "      mse_i.append(mse(ytest,yhat))\n",
        "  return np.mean(mse_i)"
      ],
      "metadata": {
        "id": "RERf4gfSXUxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_xgb=CrossMSE_xgb(x,y)\n",
        "print('The cross-validated mean square error of xgboost regressor is '+str(mse_xgb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqZ6PzoEXxGR",
        "outputId": "614165e2-dd97-4837-cb46-ff330efca08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cross-validated mean square error of xgboost regressor is 15.274251246203415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result show that xgboost greatly outperformed both locally weighted regression and gradient boosted locally weighted regression. The mean square error reported by xgboost is the lowest among all three regressors. The implementation of Gradient boost increase the accuracy of locally weighted regression. But it is not as efficient as xgboost. Since xgboost has multiple boosters while our own gradient boosting algorithm has only one booster, this result is much expected."
      ],
      "metadata": {
        "id": "8GRqpPo4LMB5"
      }
    }
  ]
}