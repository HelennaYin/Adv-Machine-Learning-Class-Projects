{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Variable Selection\n",
        "\n",
        "Variable selection is an important technique in modeling with regression algorithms. When the number of input variables exceed the number of observations, a rank deficiency problem would occur.\n",
        "\n",
        "When we use ordinary least square methods to approximate the conditional expectation of y with input variable x, we first hypothesis a model such as:\n",
        "$$ Y = \\beta X+ \\epsilon$$\n",
        "\n",
        "Here y is the vector of all dependent variables, and X is the matrix of input feature, with each row representing one observation and each column representing the same feature.\n",
        "\n",
        "We then solve this equation by multiply $X^T$ to both sides:\n",
        "\n",
        "$$X^Ty = \\beta X^TX+ \\epsilon X^T$$\n",
        "\n",
        "We then take the expected values of both sides and solve the equation, since the expected value of error term is 0, we have\n",
        "\n",
        "$$X^Ty = \\beta X^TX+ 0 X^T$$\n",
        "\n",
        "$$\\mathbb{E}(\\beta) = (X^TX)^{-1}X^TY$$\n",
        "\n",
        "When the number of input variables exceed the number of observations, X is a rank deficient matrix, which means that we cannot take the inverse of $X^T$\n",
        "\n",
        "If we tried to apply the linear regression algorithms to a rank deficient data, the model will use ordinary least square methods that minimize the mean square error.\n",
        "\n",
        "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(\\text{Residual}_i)^2$$\n",
        "\n",
        "Thus, we need variable selection to filter out features that contribute little information in predicting the dependent variable.\n",
        "\n",
        "The possible solution to the problem is Regularization. Regularization methods will put a penalty term on the loss function. Such method will put a constraint on the on the sum of the squared weights. Regularization can help construct the sparsity pattern. Progressive regularization methods will reduce some weights to zero. Then we can conclude that weights of non-zero values have stronger impacts on the prediction of dependent variable. The input variables associated with zero weights can then be excluded from the model, thus completing feature selection.\n",
        "\n",
        "In this project, I will compare the variable selection efficiency of some common regularization methods including Ridge, Lasso, ElasticNet, Square Root Lasso and Smoothly Clipped Absolute Deviation (SCAD). These methods will be tested with simulated data. Grid search algorithms will be applied to find the optimal hyperparameters that reports highest accuracy in predicting y for each algorithm. I will then compare the cross-validated accuracy, and the mean percentage of true zero and non-zero coefficients of the 10 folds to determine the performance of all those regularization methods on variable selection.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iPN5P-isIhzp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zdUt9EMCbWE"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import ceil\n",
        "from scipy import linalg\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.datasets import make_spd_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from scipy.linalg import toeplitz\n",
        "from matplotlib import pyplot\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulate Data\n",
        "\n",
        "\n",
        "For the purpose of this project, I will simulate some data with with weights of zeros for algorithms to detect. After creating the weights, I also encoded it into a binary array with 0 reports weights of 0 and 1 denoting non-zero weights."
      ],
      "metadata": {
        "id": "IYtwKpnemG-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 200\n",
        "p = 1200"
      ],
      "metadata": {
        "id": "DZPh6MKmDRdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to create beta* we need to concacenate those portions\n",
        "p1=[1]*7\n",
        "p2=[0]*25\n",
        "p3=[0.25]*5\n",
        "p4=[0]*50\n",
        "p5=[0.7]*15\n",
        "p6=[0]*1098\n",
        "beta_star = np.concatenate((p1,p2,p3,p4,p5,p6))\n",
        "beta_star_bi = np.copy(beta_star)\n",
        "beta_star_bi[beta_star_bi !=0]=1\n",
        "beta_star_bi = beta_star_bi.astype(int)"
      ],
      "metadata": {
        "id": "MbPhsQ-jD5tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simulate x, the imput variable\n",
        "x, the input variable\n",
        "\n",
        "v = []\n",
        "for i in  range(p):\n",
        "  v.append(0.8**i)\n",
        "np.random.seed(123)\n",
        "sigma = 3.5\n",
        "mu=[0]*p\n",
        "X = np.random.multivariate_normal(mu, toeplitz(v), size=n)"
      ],
      "metadata": {
        "id": "5hEn6nSPmV85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y is then created by multiplication of x and beta with random noise generated to test the algorithm function. A good feature selection algorithm should be able to identify the noise and reconstruct the sparsity pattern of beta\n",
        "\n"
      ],
      "metadata": {
        "id": "6ovt6QOfmyL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.matmul(X,beta_star) + sigma*np.random.normal(0,1,n)\n"
      ],
      "metadata": {
        "id": "bTaBLPeMmzxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions for K fold validation and reporting results\n",
        "\n",
        "I will use a 10 fold validation to report the accuracy of model by taking the mean squared errors. I will also see how many true zero values and non-zero values will be discovered by the algorithm. Since some algorithm will not reduce the coefficient to zero but rather some values close to zero, I set the threshold of weights to be 1e-05. Weights smaller than 1e-05 will taken as zero, and the algorithms will be considered as successfully filtered out that feature."
      ],
      "metadata": {
        "id": "qEaWuYvZm55X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def result_CrossVal(x,y,model):\n",
        "  mse = []\n",
        "  dist = []\n",
        "  mse_beta = []\n",
        "  dist_beta =[]\n",
        "  recall_1 = []\n",
        "  recall_0 = []\n",
        "  kf = KFold(n_splits=10,shuffle=True,random_state=123)\n",
        "  for idxtrain, idxtest in kf.split(X):\n",
        "      xtrain = X[idxtrain]\n",
        "      ytrain = y[idxtrain]\n",
        "      ytest = y[idxtest]\n",
        "      xtest = X[idxtest]\n",
        "      model.fit(X,y)\n",
        "      yhat=model.predict(X)\n",
        "      beta_hat = model.coef_\n",
        "      mse.append(MSE(y,yhat))\n",
        "      mse_beta.append(MSE(beta_star,beta_hat))\n",
        "      dist.append(np.linalg.norm(yhat-y))\n",
        "      dist_beta.append(np.linalg.norm(beta_hat-beta_star))\n",
        "      beta_bi = np.copy(beta_hat)\n",
        "      beta_bi[beta_bi >0.00001]=1\n",
        "      beta_bi[beta_bi<0.00001]=0\n",
        "      beta_bi[beta_bi==0.00001]=1\n",
        "      beta_bi = beta_bi.astype(int)\n",
        "      report= classification_report(beta_star_bi,beta_bi,output_dict = True)\n",
        "      recall_1.append(report['1']['recall'])\n",
        "      recall_0.append(report['0']['recall'])\n",
        "\n",
        "\n",
        "  return np.mean(mse),np.mean(dist),np.mean(recall_1),np.mean(recall_0),np.mean(mse_beta),np.mean(dist_beta)"
      ],
      "metadata": {
        "id": "b9D_aV4Pvwke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_statment(X,name):\n",
        "  print(\"The mean squared error of true y and predicted y is %s\" %(X[0]))\n",
        "  print(\"The euclidean distance between true y and predicted y is %s\" %(X[1]))\n",
        "  print(\"The percentage of true non-zero coefficients  discovered is %s \"%(X[2]))\n",
        "  print(\"The percentage of true zero coefficients  discovered by %s \"%(X[3]))\n",
        "  print(\"The mean squared error of true coefficients and predicted coefficients is %s\" %(X[4]))\n",
        "  print(\"The euclidean distance between true true coefficients and predicted coefficients is %s\" %(X[5]))\n"
      ],
      "metadata": {
        "id": "5RjyOiKaPtij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Selection with Lasso"
      ],
      "metadata": {
        "id": "byOl524Ougg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variable selection with Lasso\n",
        "Lasso is also know as L1 norm. The penalty term of L1 norm is the sum of absolute values of weights. The least square function with added penalty term will be:\n",
        "\n",
        "$$\\frac{1}{n} \\sum_{i=1}^{n} (\\text{Residual}_i)^2 + \\alpha \\|\\beta\\|_1\n",
        "$$\n",
        "\n",
        "alpha is the hyperparameter, it specify how much we penalizing the function to minimize. Grid search methods will be applied to estimate the optimal alpha value. We will use algorithm imported from scikit-learn library\n",
        "\n"
      ],
      "metadata": {
        "id": "ygsOt66SnCf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasreg = Lasso(fit_intercept = False,max_iter = 10000)\n",
        "params = [{'alpha':np.linspace(0.001,1,num=200)}]\n",
        "gs = GridSearchCV(estimator=lasreg,cv=10,scoring='r2',param_grid=params)\n",
        "gs_results = gs.fit(X,y)\n",
        "alpha_lasso = gs_results.best_params_.get('alpha')\n",
        "model = Lasso(alpha = alpha_lasso,fit_intercept = False, max_iter = 10000)\n",
        "A=result_CrossVal(X,y,model)"
      ],
      "metadata": {
        "id": "6TzynoLXspsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_statment(A,\"Lasso\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSbhGDOvPkFy",
        "outputId": "6ae5e658-1804-48f9-d88d-81f8c88b859e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean squared error of true y and predicted y is 14.95926466392552\n",
            "The euclidean distance between true y and predicted y is 2600.4266905242175\n",
            "The percentage of true non-zero coefficients  discovered is 0.7037037037037038 \n",
            "The percentage of true zero coefficients  discovered by 1.0 \n",
            "The mean squared error of true coefficients and predicted coefficients is 0.007456596759708996\n",
            "The euclidean distance between true true coefficients and predicted coefficients is 2.991306756528122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lasso algorithm successfully found out all the zeros in the weights. However, it might have taken a rather radical step that some non-zero weights were also reduced to zero."
      ],
      "metadata": {
        "id": "sipxhPqjpKCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Selection with Ridge\n",
        "\n",
        "Ridge is also know as L2 norm. The penalty term of L2 norm is the sum of square root of weights. The least square function with added penalty term will be:\n",
        "\n",
        "$$\\frac{1}{n} \\sum_{i=1}^{n} (\\text{Residual}_i)^2 + \\alpha \\sum_{j=1}^{p} \\beta_j^2$$\n",
        "\n",
        "\n",
        "alpha is the hyperparameter, it specify how much we penalizing the function to minimize. Grid search methods will be applied to estimate the optimal alpha value. I will use algorithm imported from scikit-learn library"
      ],
      "metadata": {
        "id": "hukIx_Wu2I8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ridgereg = Ridge(fit_intercept = False,max_iter = 10000)\n",
        "params = [{'alpha':np.linspace(0.1,0.001,num=100)}]\n",
        "gs = GridSearchCV(estimator=Ridgereg,cv=10,scoring='r2',param_grid=params)\n",
        "gs_results = gs.fit(X,y)\n",
        "alpha_R = gs_results.best_params_.get('alpha')\n"
      ],
      "metadata": {
        "id": "lg0lxpqG2F_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ridreg = Ridge(alpha = alpha_R,fit_intercept = False,max_iter = 10000)\n",
        "R_result=result_CrossVal(X,y,Ridreg)\n",
        "print_statment(R_result,\"Ridge\")"
      ],
      "metadata": {
        "id": "GcpxlqwG2Rrq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda28404-8170-4961-cd46-e77bc3f274fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean squared error of true y and predicted y is 9.05992338232221e-11\n",
            "The euclidean distance between true y and predicted y is 0.0001346099801821708\n",
            "The percentage of true non-zero coefficients  discovered is 1.0 \n",
            "The percentage of true zero coefficients  discovered by 0.5251491901108271 \n",
            "The mean squared error of true coefficients and predicted coefficients is 0.007514031264213061\n",
            "The euclidean distance between true true coefficients and predicted coefficients is 3.002804941559753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge is not as efficient as Lasso on feature selection, it only discovers 52.5% of the zeros in weights. However, ridge will avoid making mistakes in recognizing non-zero values into zeros."
      ],
      "metadata": {
        "id": "ZE7DQYnip1ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Selection with ElasticNet\n",
        "\n",
        "ElasticNet combines L1 and L2 regularizations. The penalty term combines the absolute value of weights and the squared terms of weights, so the function with mean squared error to be minimize is:\n",
        "\n",
        "$$\\frac{1}{n} \\sum_{i=1}^{n} (\\text{Residual}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| + (1 - \\lambda) \\cdot \\sum_{j=1}^{p} \\beta_j^2$$\n"
      ],
      "metadata": {
        "id": "qj9EujEYOO-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENreg = ElasticNet(fit_intercept = False,max_iter = 10000)\n",
        "params = [{'alpha':np.linspace(0.01,0.1,num=10),'l1_ratio':np.linspace(0.1,1,num=10)}]\n",
        "gs = GridSearchCV(estimator=ENreg,cv=10,scoring='r2',param_grid=params)\n",
        "gs_results = gs.fit(X,y)\n",
        "alpha_EN = gs_results.best_params_.get('alpha')\n",
        "l1_EN = gs_results.best_params_.get('l1_ratio')\n",
        "model = ElasticNet(alpha = alpha_EN,l1_ratio = l1_EN, fit_intercept = False, max_iter = 10000)\n",
        "A=result_CrossVal(x,y,model)"
      ],
      "metadata": {
        "id": "jyZaYkzfOTpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_statment(A,\"ElasticNet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU1br7g0mR7m",
        "outputId": "2c6cc455-fecb-4462-b928-c84eefe2bcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean squared error of true y and predicted y is 0.16777371964906712\n",
            "The euclidean distance between true y and predicted y is 2882.7721156410175\n",
            "The percentage of true non-zero coefficients  discovered is 1.0 \n",
            "The percentage of true zero coefficients  discovered by 0.8312020460358056 \n",
            "The mean squared error of true coefficients and predicted coefficients is 0.004902890785113761\n",
            "The euclidean distance between true true coefficients and predicted coefficients is 2.4255863089439873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic performs better on defining the sparsity pattern of weights, as it discovers 83% of true zero coefficient without reduce any non-zero coefficients to zero. Although it did not performs as progressive in feature selection as Lasso regression, note that the accuracy of predicted y denoted by mean square error is lower than that of Lasso."
      ],
      "metadata": {
        "id": "wgTMkdJOsSDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Selection with Square Root Lasso\n",
        "\n",
        "Square root lasso is modification of Lasso regularization, specially developed to estimate high-dimensional sparse linear regression. The objective function we want to minimize with penalty is\n",
        "\n",
        "$$\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\text{Residual}_i)^2} + \\alpha \\sum_{i=1}^{n} |\\beta_i|$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZWe1edKomh2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SQRTLasso(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, alpha=0.01):\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        alpha=self.alpha\n",
        "\n",
        "        def f_obj(x,y,beta,alpha):\n",
        "          n =len(x)\n",
        "          beta = beta.flatten()\n",
        "          beta = beta.reshape(-1,1)\n",
        "          output = np.sqrt(1/n*np.sum((y-x.dot(beta))**2)) + alpha*np.sum(np.abs(beta))\n",
        "          return output\n",
        "\n",
        "        def f_grad(x,y,beta,alpha):\n",
        "          n=x.shape[0]\n",
        "          p=x.shape[1]\n",
        "          beta = beta.flatten()\n",
        "          beta = beta.reshape(-1,1)\n",
        "          output = (-1/np.sqrt(n))*np.transpose(x).dot(y-x.dot(beta))/np.sqrt(np.sum((y-x.dot(beta))**2))+alpha*np.sign(beta)\n",
        "          return output.flatten()\n",
        "\n",
        "        def objective(beta):\n",
        "          return(f_obj(x,y,beta,alpha))\n",
        "\n",
        "        def gradient(beta):\n",
        "          return(f_grad(x,y,beta,alpha))\n",
        "\n",
        "        beta0 = np.ones((x.shape[1],1))\n",
        "        output = minimize(objective, beta0, method='L-BFGS-B', jac=gradient,options={'gtol': 1e-8, 'maxiter': 50000,'maxls': 25,'disp': True})\n",
        "        beta = output.x\n",
        "        self.coef_ = beta\n",
        "    def coef_(self):\n",
        "      return self.coef_\n",
        "\n",
        "    def predict(self, x):\n",
        "        return x.dot(self.coef_)\n",
        "\n"
      ],
      "metadata": {
        "id": "jKGucHnVmhMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "x = np.random.multivariate_normal(mu, toeplitz(v), size=n)\n",
        "y = np.matmul(x,beta_star).reshape(-1,1) + sigma*np.random.normal(0,1,size=(n,1))\n"
      ],
      "metadata": {
        "id": "lj8BsX0KYugj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sqrtLasso = SQRTLasso()\n",
        "params = [{'alpha':np.linspace(0.1,1,num=10)}]\n",
        "gs = GridSearchCV(estimator=sqrtLasso,cv=10,scoring='r2',param_grid=params)\n",
        "gs_results = gs.fit(X,y)\n",
        "alpha_sqrtLasso  = gs_results.best_params_.get('alpha')"
      ],
      "metadata": {
        "id": "JebT0WfEpR0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SQRTLasso(alpha = 0.2)\n",
        "A=result_CrossVal(X,y,model)\n",
        "print_statment(A,\"SQRTLasso\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2CYI7nQeoKT",
        "outputId": "1f19ddfe-619e-4403-e377-02f0e4843402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean squared error of true y and predicted y is 13.976022056637756\n",
            "The euclidean distance between true y and predicted y is 2636.310778214968\n",
            "The percentage of true non-zero coefficients  discovered is 0.962962962962963 \n",
            "The percentage of true zero coefficients  discovered by 0.988917306052856 \n",
            "The mean squared error of true coefficients and predicted coefficients is 0.0012223481215289157\n",
            "The euclidean distance between true true coefficients and predicted coefficients is 1.211122514791422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Square root lasso yields both high percentage of correctly classified zero and non-zero weights. The percentage of true non-zero weights discovered is 96.3% and that of true zero weights discovered is 98.9%. It outperforms Lasso on correctly locating the zero coefficients without falsely reduced non-zero betas to zero.\n",
        "\n"
      ],
      "metadata": {
        "id": "qb6QvhL2ty6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Selection with SCAD\n",
        "\n",
        "The smoothly clipped absolute deviation is a methods developed to address the problem of introducing bias by LASSO algorithm. The penalty term $p(\\beta)$ for SCAD is defined by its derivative\n",
        "\n",
        "$$p'_\\lambda(\\beta) = \\lambda \\left\\{ I(\\beta \\leq \\lambda) + \\frac{\\alpha \\lambda - \\beta}{(\\alpha - 1)\\lambda} + I(\\beta > \\lambda) \\right\\}\n",
        "$$\n",
        "\n",
        "$\\alpha$ and $\\lambda$ are hyperparameters that controls the penalty. We can see from this algorithm that it will not penalize larger value of $\\beta$. For small values of $\\beta$, it will be penalized by $\\lambda|\\beta|$. For $\\beta$ between $\\lambda$ and $\\alpha\\lambda$, the objective function will be penalized by $\\frac{\\alpha \\lambda |\\beta| - \\beta^2 - \\lambda^2}{\\alpha - 1}$\n",
        ""
      ],
      "metadata": {
        "id": "HZAtAnvjh18m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SCAD(BaseEstimator, RegressorMixin):\n",
        "  def __init__(self, a=1,lam=1):\n",
        "    self.a = a\n",
        "    self.lam = lam\n",
        "\n",
        "  def fit(self, x, y):\n",
        "      a=self.a\n",
        "      lam = self.lam\n",
        "\n",
        "\n",
        "      n = x.shape[0]\n",
        "      p = x.shape[1]\n",
        "      def scad_penalty(beta_hat, lambda_val, a_val):\n",
        "          is_linear = (np.abs(beta_hat) <= lambda_val)\n",
        "          is_quadratic = np.logical_and(lambda_val < np.abs(beta_hat), np.abs(beta_hat) <= a_val * lambda_val)\n",
        "          is_constant = (a_val * lambda_val) < np.abs(beta_hat)\n",
        "\n",
        "          linear_part = lambda_val * np.abs(beta_hat) * is_linear\n",
        "          quadratic_part = (2 * a_val * lambda_val * np.abs(beta_hat) - beta_hat**2 - lambda_val**2) / (2 * (a_val - 1)) * is_quadratic\n",
        "          constant_part = (lambda_val**2 * (a_val + 1)) / 2 * is_constant\n",
        "          return linear_part + quadratic_part + constant_part\n",
        "\n",
        "      def scad_derivative(beta_hat, lambda_val, a_val):\n",
        "          return lambda_val * ((beta_hat <= lambda_val) + (a_val * lambda_val - beta_hat)*((a_val * lambda_val - beta_hat) > 0) / ((a_val - 1) * lambda_val) * (beta_hat > lambda_val))\n",
        "\n",
        "\n",
        "      def scad(beta,x,y):\n",
        "          beta = beta.flatten()\n",
        "          beta = beta.reshape(-1,1)\n",
        "          n = len(y)\n",
        "          return 1/n*np.sum((y-x.dot(beta))**2) + np.sum(scad_penalty(beta,lam,a))\n",
        "\n",
        "      def dscad(beta,x,y):\n",
        "          beta = beta.flatten()\n",
        "          beta = beta.reshape(-1,1)\n",
        "          n = len(y)\n",
        "          return np.array(-2/n*np.transpose(x).dot(y-x.dot(beta))+scad_derivative(beta,lam,a)).flatten()\n",
        "\n",
        "      def objective(beta):\n",
        "          return scad(beta,x,y)\n",
        "\n",
        "      def gradient(beta):\n",
        "          return dscad(beta,x,y)\n",
        "\n",
        "      b0 = np.ones((p,1))\n",
        "      output = minimize(objective, b0, method='L-BFGS-B', jac=gradient,options={'gtol': 1e-8, 'maxiter': 1e7,'maxls': 25,'disp': True})\n",
        "      beta = output.x\n",
        "      self.coef_ = beta\n",
        "\n",
        "  def coef_(self):\n",
        "    return self.coef_\n",
        "\n",
        "  def predict(self, x):\n",
        "    return x.dot(self.coef_)\n"
      ],
      "metadata": {
        "id": "5FeQMtELhztk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "x = np.random.multivariate_normal(mu, toeplitz(v), size=n)\n",
        "y = np.matmul(x,beta_star).reshape(-1,1) + sigma*np.random.normal(0,1,size=(n,1))"
      ],
      "metadata": {
        "id": "83nRyCLpt4Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SCADreg = SCAD()\n",
        "params = [{'a':np.linspace(10,1,num=10),'lam':np.linspace(1,0.1,num=10)}]\n",
        "gs = GridSearchCV(estimator=SCADreg,cv=10,scoring='neg_mean_squared_error',param_grid=params)\n",
        "gs_results = gs.fit(X,y)\n",
        "a_SCAD= gs_results.best_params_.get('a')\n",
        "lam_SCAD = gs_results.best_params_.get('lam')"
      ],
      "metadata": {
        "id": "NfvEZ2yOtbrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SCAD(a=a_SCAD,lam=lam_SCAD)\n",
        "A=result_CrossVal(X,y,model)\n",
        "print_statment(A,\"SCAD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXCUsqC7wfi8",
        "outputId": "64f5a50f-cf3a-4866-8b99-824f3a7cc557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean squared error of true y and predicted y is 3.604251976224318\n",
            "The euclidean distance between true y and predicted y is 2919.161486375872\n",
            "The percentage of true non-zero coefficients  discovered is 1.0 \n",
            "The percentage of true zero coefficients  discovered by 0.6683716965046889 \n",
            "The mean squared error of true coefficients and predicted coefficients is 0.014587625433907228\n",
            "The euclidean distance between true true coefficients and predicted coefficients is 4.183915692349532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SCAD function didn't performed well on feature selection. The percentage of true zero weights it discovered, 66.8% are more than those of Ridge, but less than others methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "8JKDtfOavnQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "Among the regularization methods we experimented with simulated dataset, Square Root Lasso performed the best with feature selection and constructing the sparsity pattern of weights."
      ],
      "metadata": {
        "id": "fEUgud-2vs6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "\n",
        " https://andrewcharlesjones.github.io/journal/scad.html Belloni, A., Chernozhukov, V., & Wang, L. (2011). Square-root lasso: Pivotal recovery of sparse signals via conic programming. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.1910753"
      ],
      "metadata": {
        "id": "6IeTu7NGvw78"
      }
    }
  ]
}